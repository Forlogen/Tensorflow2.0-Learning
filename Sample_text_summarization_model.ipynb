{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3.5.20\\envs\\tensorflow2.0\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "D:\\Anaconda3.5.20\\envs\\tensorflow2.0\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "D:\\Anaconda3.5.20\\envs\\tensorflow2.0\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "D:\\Anaconda3.5.20\\envs\\tensorflow2.0\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "D:\\Anaconda3.5.20\\envs\\tensorflow2.0\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "D:\\Anaconda3.5.20\\envs\\tensorflow2.0\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "D:\\Anaconda3.5.20\\envs\\tensorflow2.0\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "D:\\Anaconda3.5.20\\envs\\tensorflow2.0\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "D:\\Anaconda3.5.20\\envs\\tensorflow2.0\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "D:\\Anaconda3.5.20\\envs\\tensorflow2.0\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "D:\\Anaconda3.5.20\\envs\\tensorflow2.0\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "D:\\Anaconda3.5.20\\envs\\tensorflow2.0\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0-beta0\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import,division,print_function,unicode_literals\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import re\n",
    "import collections\n",
    "import tensorflow_datasets as tfds\n",
    "print (tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(22)\n",
    "np.random.seed(22)\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_article_path = \"sumdata/train/train.article.txt\"\n",
    "train_title_path = \"sumdata/train/train.title.txt\"\n",
    "valid_article_path = \"sumdata/train/valid.article.filter.txt\"\n",
    "valid_title_path = \"sumdata/train/valid.title.filter.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_hidden = 1024\n",
    "num_layers = 2\n",
    "beam_size = 10\n",
    "embedding_size = 128\n",
    "lstm_size = 1024\n",
    "learning_rate = 1e-3\n",
    "batch_size = 10\n",
    "buffer_size = 32\n",
    "num_epochs = 20\n",
    "keep_prob = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text(text):\n",
    "    # create a space between a word and the punctuation following it\n",
    "    text = re.sub(r\"([?.!,¿])\", r\" \\1 \", text)\n",
    "    text = re.sub(r'[\" \"]+', \" \", text)\n",
    "    # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n",
    "    text = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", text)\n",
    "    text = text.strip()\n",
    "    return text  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path):\n",
    "    text = []\n",
    "    \n",
    "    with open(path,encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            text_ = str(line.strip().split('\\t'))\n",
    "            text.append(process_text(text_))\n",
    "    return text          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of article dataset is: 100\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['australia s current account deficit shrunk by a record . billion dollars lrb . billion us rrb in the june quarter due to soaring commodity prices , figures released monday showed .',\n",
       " 'at least two people were killed in a suspected bomb attack on a passenger bus in the strife torn southern philippines on monday , the military said .',\n",
       " 'australian shares closed down . percent monday following a weak lead from the united states and lower commodity prices , dealers said .',\n",
       " 'south korea s nuclear envoy kim sook urged north korea monday to restart work to disable its nuclear plants and stop its typical brinkmanship in negotiations .',\n",
       " 'south korea on monday announced sweeping tax reforms , including income and corporate tax cuts to boost growth by stimulating sluggish private consumption and business investment .']"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article = load_data(train_article_path)\n",
    "print ('The length of article dataset is: {}'.format(len(article)))\n",
    "article[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of title dataset is: 100\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['australian current account deficit narrows sharply',\n",
       " 'at least two dead in southern philippines blast',\n",
       " 'australian stocks close down . percent',\n",
       " 'envoy urges north korea to restart nuclear disablement',\n",
       " 'skorea announces tax cuts to stimulate economy']"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "title = load_data(train_title_path)\n",
    "print ('The length of title dataset is: {}'.format(len(title)))\n",
    "title[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The cocabulary size of word dict is: 1185\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'the': 0,\n",
       " '.': 1,\n",
       " ',': 2,\n",
       " 'in': 3,\n",
       " 'a': 4,\n",
       " 'of': 5,\n",
       " 'to': 6,\n",
       " 'on': 7,\n",
       " 'wednesday': 8,\n",
       " 'and': 9,\n",
       " 's': 10,\n",
       " 'said': 11,\n",
       " 'us': 12,\n",
       " 'monday': 13,\n",
       " 'for': 14,\n",
       " 'as': 15,\n",
       " 'its': 16,\n",
       " 'by': 17,\n",
       " 'prices': 18,\n",
       " 'an': 19,\n",
       " 'percent': 20,\n",
       " 'against': 21,\n",
       " 'at': 22,\n",
       " 'it': 23,\n",
       " 'down': 24,\n",
       " 'new': 25,\n",
       " 'but': 26,\n",
       " 'from': 27,\n",
       " 'with': 28,\n",
       " 'close': 29,\n",
       " 'over': 30,\n",
       " 'shares': 31,\n",
       " 'south': 32,\n",
       " 'that': 33,\n",
       " 'oil': 34,\n",
       " 'has': 35,\n",
       " 'is': 36,\n",
       " 'after': 37,\n",
       " 'president': 38,\n",
       " 'dollars': 39,\n",
       " 'southern': 40,\n",
       " 'gulf': 41,\n",
       " 'up': 42,\n",
       " 'two': 43,\n",
       " 'closed': 44,\n",
       " 'dealers': 45,\n",
       " 'korea': 46,\n",
       " 'state': 47,\n",
       " 'was': 48,\n",
       " 'obama': 49,\n",
       " 'lower': 50,\n",
       " 'street': 51,\n",
       " 'russia': 52,\n",
       " 'not': 53,\n",
       " 'georgia': 54,\n",
       " 'were': 55,\n",
       " 'wall': 56,\n",
       " 'chief': 57,\n",
       " 'lehman': 58,\n",
       " 'hurricane': 59,\n",
       " 'trade': 60,\n",
       " 'raids': 61,\n",
       " 'bolivia': 62,\n",
       " 'ambassador': 63,\n",
       " 'zimbabwe': 64,\n",
       " 'deal': 65,\n",
       " 'powerful': 66,\n",
       " 'due': 67,\n",
       " 'envoy': 68,\n",
       " 'share': 69,\n",
       " 'losses': 70,\n",
       " 'which': 71,\n",
       " 'amid': 72,\n",
       " 'bank': 73,\n",
       " 'police': 74,\n",
       " 'no': 75,\n",
       " 'world': 76,\n",
       " 'power': 77,\n",
       " 'army': 78,\n",
       " 'country': 79,\n",
       " 'euro': 80,\n",
       " 'texas': 81,\n",
       " 'stocks': 82,\n",
       " 'billion': 83,\n",
       " 'people': 84,\n",
       " 'australian': 85,\n",
       " 'would': 86,\n",
       " 'zealand': 87,\n",
       " 'higher': 88,\n",
       " 'investors': 89,\n",
       " 'hong': 90,\n",
       " 'kong': 91,\n",
       " 'one': 92,\n",
       " 'pakistan': 93,\n",
       " 'talks': 94,\n",
       " 'ike': 95,\n",
       " 'official': 96,\n",
       " 'security': 97,\n",
       " 'visit': 98,\n",
       " 'mugabe': 99,\n",
       " 'iran': 100,\n",
       " 'judge': 101,\n",
       " 'prosecutor': 102,\n",
       " 'killed': 103,\n",
       " 'military': 104,\n",
       " 'following': 105,\n",
       " 'announced': 106,\n",
       " 'investment': 107,\n",
       " 'giant': 108,\n",
       " 'first': 109,\n",
       " 'authorities': 110,\n",
       " 'party': 111,\n",
       " 'charged': 112,\n",
       " 'year': 113,\n",
       " 'philippine': 114,\n",
       " 'morning': 115,\n",
       " 'fell': 116,\n",
       " 'troops': 117,\n",
       " 'all': 118,\n",
       " 'month': 119,\n",
       " 'week': 120,\n",
       " 'island': 121,\n",
       " 'are': 122,\n",
       " 'government': 123,\n",
       " 'european': 124,\n",
       " 'will': 125,\n",
       " 'without': 126,\n",
       " 'barack': 127,\n",
       " 'mccain': 128,\n",
       " 'hold': 129,\n",
       " 'recession': 130,\n",
       " 'his': 131,\n",
       " 'trafficking': 132,\n",
       " 'eu': 133,\n",
       " 'both': 134,\n",
       " 'dead': 135,\n",
       " 'day': 136,\n",
       " 'five': 137,\n",
       " 'un': 138,\n",
       " 'commodity': 139,\n",
       " 'united': 140,\n",
       " 'states': 141,\n",
       " 'north': 142,\n",
       " 'negotiations': 143,\n",
       " 'taiwan': 144,\n",
       " 'leader': 145,\n",
       " 'political': 146,\n",
       " 'into': 147,\n",
       " 'john': 148,\n",
       " 'some': 149,\n",
       " 'measures': 150,\n",
       " 'dollar': 151,\n",
       " 'analysts': 152,\n",
       " 'brothers': 153,\n",
       " 'rate': 154,\n",
       " 'august': 155,\n",
       " 'court': 156,\n",
       " 'officials': 157,\n",
       " 'little': 158,\n",
       " 'changed': 159,\n",
       " 'gustav': 160,\n",
       " 'coast': 161,\n",
       " 'ended': 162,\n",
       " 'friday': 163,\n",
       " 'news': 164,\n",
       " 'reported': 165,\n",
       " 'last': 166,\n",
       " 'earthquake': 167,\n",
       " 'there': 168,\n",
       " 'mexico': 169,\n",
       " 'resort': 170,\n",
       " 'put': 171,\n",
       " 'summit': 172,\n",
       " 'since': 173,\n",
       " 'forces': 174,\n",
       " 'more': 175,\n",
       " 'next': 176,\n",
       " 'opposition': 177,\n",
       " 'lipstick': 178,\n",
       " 'below': 179,\n",
       " 'time': 180,\n",
       " 'eurozone': 181,\n",
       " 'charges': 182,\n",
       " 'deadly': 183,\n",
       " 'category': 184,\n",
       " 'storm': 185,\n",
       " 'thursday': 186,\n",
       " 'russian': 187,\n",
       " 'minister': 188,\n",
       " 'sharing': 189,\n",
       " 'tsvangirai': 190,\n",
       " 'sell': 191,\n",
       " 'assets': 192,\n",
       " 'presidential': 193,\n",
       " 'west': 194,\n",
       " 'israel': 195,\n",
       " 'execution': 196,\n",
       " 'skorea': 197,\n",
       " 'quake': 198,\n",
       " 'accuses': 199,\n",
       " 'deficit': 200,\n",
       " 'lrb': 201,\n",
       " 'rrb': 202,\n",
       " 'least': 203,\n",
       " 'attack': 204,\n",
       " 'philippines': 205,\n",
       " 'nuclear': 206,\n",
       " 'urged': 207,\n",
       " 'tax': 208,\n",
       " 'reforms': 209,\n",
       " 'weakness': 210,\n",
       " 'spanish': 211,\n",
       " 'he': 212,\n",
       " 'holiday': 213,\n",
       " 'decade': 214,\n",
       " 'made': 215,\n",
       " 'won': 216,\n",
       " 'decline': 217,\n",
       " 'british': 218,\n",
       " 'troubled': 219,\n",
       " 'delays': 220,\n",
       " 'man': 221,\n",
       " 'awaited': 222,\n",
       " 'early': 223,\n",
       " 'sharply': 224,\n",
       " 'rebel': 225,\n",
       " 'accused': 226,\n",
       " 'opened': 227,\n",
       " 'than': 228,\n",
       " 'union': 229,\n",
       " 'relations': 230,\n",
       " 'emergency': 231,\n",
       " 'general': 232,\n",
       " 'ashfaq': 233,\n",
       " 'kayani': 234,\n",
       " 'strongly': 235,\n",
       " 'criticised': 236,\n",
       " 'cross': 237,\n",
       " 'border': 238,\n",
       " 'led': 239,\n",
       " 'coalition': 240,\n",
       " 'defend': 241,\n",
       " 'sovereignty': 242,\n",
       " 'cost': 243,\n",
       " 'eon': 244,\n",
       " 'immigration': 245,\n",
       " 'western': 246,\n",
       " 'white': 247,\n",
       " 'house': 248,\n",
       " 'democrat': 249,\n",
       " 'race': 250,\n",
       " 'talk': 251,\n",
       " 'economy': 252,\n",
       " 'evo': 253,\n",
       " 'morales': 254,\n",
       " 'ordered': 255,\n",
       " 'expelled': 256,\n",
       " 'accusing': 257,\n",
       " 'him': 258,\n",
       " 'war': 259,\n",
       " 'fears': 260,\n",
       " 'unk': 261,\n",
       " 'says': 262,\n",
       " 'word': 263,\n",
       " 'declared': 264,\n",
       " 'strengthened': 265,\n",
       " 'toward': 266,\n",
       " 'robert': 267,\n",
       " 'attacks': 268,\n",
       " 'plans': 269,\n",
       " 'off': 270,\n",
       " 'shore': 271,\n",
       " 'finances': 272,\n",
       " 'hefty': 273,\n",
       " 'jolted': 274,\n",
       " 'killing': 275,\n",
       " 'damaging': 276,\n",
       " 'scores': 277,\n",
       " 'homes': 278,\n",
       " 'strategic': 279,\n",
       " 'waters': 280,\n",
       " 'candidate': 281,\n",
       " 'biggest': 282,\n",
       " 'sarkozy': 283,\n",
       " 'planned': 284,\n",
       " 'convicted': 285,\n",
       " 'ruling': 286,\n",
       " 'fight': 287,\n",
       " 'criticises': 288,\n",
       " 'orders': 289,\n",
       " 'falls': 290,\n",
       " 'hits': 291,\n",
       " 'current': 292,\n",
       " 'account': 293,\n",
       " 'released': 294,\n",
       " 'showed': 295,\n",
       " 'suspected': 296,\n",
       " 'bomb': 297,\n",
       " 'weak': 298,\n",
       " 'lead': 299,\n",
       " 'restart': 300,\n",
       " 'cuts': 301,\n",
       " 'growth': 302,\n",
       " 'business': 303,\n",
       " 'lacklustre': 304,\n",
       " 'interim': 305,\n",
       " 'earnings': 306,\n",
       " 'electronics': 307,\n",
       " 'manufacturing': 308,\n",
       " 'hon': 309,\n",
       " 'hai': 310,\n",
       " 'group': 311,\n",
       " 'colonial': 312,\n",
       " 'under': 313,\n",
       " 'euros': 314,\n",
       " 'half': 315,\n",
       " 'kadhafi': 316,\n",
       " 'wide': 317,\n",
       " 'economic': 318,\n",
       " 'nations': 319,\n",
       " 'arrived': 320,\n",
       " 'ethiopia': 321,\n",
       " 'regions': 322,\n",
       " 'drought': 323,\n",
       " 'aid': 324,\n",
       " 'beijing': 325,\n",
       " 'best': 326,\n",
       " 'air': 327,\n",
       " 'steps': 328,\n",
       " 'push': 329,\n",
       " 'korean': 330,\n",
       " 'steep': 331,\n",
       " 'continued': 332,\n",
       " 'run': 333,\n",
       " 'newspaper': 334,\n",
       " 'inflation': 335,\n",
       " 'slightly': 336,\n",
       " 'national': 337,\n",
       " 'parliament': 338,\n",
       " 'began': 339,\n",
       " 'session': 340,\n",
       " 'through': 341,\n",
       " 'cambodia': 342,\n",
       " 'child': 343,\n",
       " 'impact': 344,\n",
       " 'slammed': 345,\n",
       " 'tracking': 346,\n",
       " 'finance': 347,\n",
       " 'bill': 348,\n",
       " 'price': 349,\n",
       " 'barrel': 350,\n",
       " 'here': 351,\n",
       " 'policemen': 352,\n",
       " 'ingushetia': 353,\n",
       " 'agency': 354,\n",
       " 'late': 355,\n",
       " 'operations': 356,\n",
       " 'muslim': 357,\n",
       " 'leaders': 358,\n",
       " 'they': 359,\n",
       " 'ramadan': 360,\n",
       " 'chinese': 361,\n",
       " 'market': 362,\n",
       " 'moderate': 363,\n",
       " 'central': 364,\n",
       " 'or': 365,\n",
       " 'asian': 366,\n",
       " 'forced': 367,\n",
       " 'almost': 368,\n",
       " 'stranded': 369,\n",
       " 'phuket': 370,\n",
       " 'expected': 371,\n",
       " 'home': 372,\n",
       " 'high': 373,\n",
       " 'gold': 374,\n",
       " 'junta': 375,\n",
       " 'formation': 376,\n",
       " 'malaysia': 377,\n",
       " 'markets': 378,\n",
       " 'winter': 379,\n",
       " 'catholic': 380,\n",
       " 'bishops': 381,\n",
       " 'election': 382,\n",
       " 'win': 383,\n",
       " 'major': 384,\n",
       " 'pollster': 385,\n",
       " 'popular': 386,\n",
       " 'decisive': 387,\n",
       " 'plumbed': 388,\n",
       " 'depths': 389,\n",
       " 'stooping': 390,\n",
       " 'lies': 391,\n",
       " 'phony': 392,\n",
       " 'outrage': 393,\n",
       " 'response': 394,\n",
       " 'mocking': 395,\n",
       " 'pig': 396,\n",
       " 'china': 397,\n",
       " 'california': 398,\n",
       " 'concerns': 399,\n",
       " 'about': 400,\n",
       " 'slowing': 401,\n",
       " 'global': 402,\n",
       " 'warned': 403,\n",
       " 'headed': 404,\n",
       " 'towards': 405,\n",
       " 'civil': 406,\n",
       " 'demand': 407,\n",
       " 'britain': 408,\n",
       " 'prince': 409,\n",
       " 'charles': 410,\n",
       " 'called': 411,\n",
       " 'wartime': 412,\n",
       " 'facebook': 413,\n",
       " 'devotees': 414,\n",
       " 'department': 415,\n",
       " 'had': 416,\n",
       " 'received': 417,\n",
       " 'formal': 418,\n",
       " 'expelling': 419,\n",
       " 'baseless': 420,\n",
       " 'bolivian': 421,\n",
       " 'la': 422,\n",
       " 'paz': 423,\n",
       " 'persona': 424,\n",
       " 'non': 425,\n",
       " 'grata': 426,\n",
       " 'encouraging': 427,\n",
       " 'breakup': 428,\n",
       " 'promoting': 429,\n",
       " 'separatism': 430,\n",
       " 'evacuations': 431,\n",
       " 'cut': 432,\n",
       " 'tuareg': 433,\n",
       " 'hostages': 434,\n",
       " 'mali': 435,\n",
       " 'cooperation': 436,\n",
       " 'osce': 437,\n",
       " 'member': 438,\n",
       " 'efforts': 439,\n",
       " 'crack': 440,\n",
       " 'human': 441,\n",
       " 'foreign': 442,\n",
       " 'lavrov': 443,\n",
       " 'weakened': 444,\n",
       " 'prospects': 445,\n",
       " 'morgan': 446,\n",
       " 'hinted': 447,\n",
       " 'agreement': 448,\n",
       " 'very': 449,\n",
       " 'mayor': 450,\n",
       " 'center': 451,\n",
       " 'six': 452,\n",
       " 'wobbled': 453,\n",
       " 'mulled': 454,\n",
       " 'retrial': 455,\n",
       " 'reach': 456,\n",
       " 'truce': 457,\n",
       " 'appeared': 458,\n",
       " 'prime': 459,\n",
       " 'denied': 460,\n",
       " 'article': 461,\n",
       " 'kenyan': 462,\n",
       " 'grandmother': 463,\n",
       " 'france': 464,\n",
       " 'challenges': 465,\n",
       " 'arctic': 466,\n",
       " 'soldiers': 467,\n",
       " 'printer': 468,\n",
       " 'out': 469,\n",
       " 'dispute': 470,\n",
       " 'mining': 471,\n",
       " 'armed': 472,\n",
       " 'sierra': 473,\n",
       " 'leone': 474,\n",
       " 'youngest': 475,\n",
       " 'french': 476,\n",
       " 'fiefdom': 477,\n",
       " 'gul': 478,\n",
       " 'azerbaijan': 479,\n",
       " 'armenia': 480,\n",
       " 'israeli': 481,\n",
       " 'helicopter': 482,\n",
       " 'northern': 483,\n",
       " 'nato': 484,\n",
       " 'cancel': 485,\n",
       " 'ambassadors': 486,\n",
       " 'churned': 487,\n",
       " 'strike': 488,\n",
       " 'boeing': 489,\n",
       " 'fifth': 490,\n",
       " 'drug': 491,\n",
       " 'internet': 492,\n",
       " 'experts': 493,\n",
       " 'palestinian': 494,\n",
       " 'raid': 495,\n",
       " 'pentagon': 496,\n",
       " 'tanker': 497,\n",
       " 'paedophile': 498,\n",
       " 'remain': 499,\n",
       " 'free': 500,\n",
       " 'before': 501,\n",
       " 'foiled': 502,\n",
       " 'abroad': 503,\n",
       " 'editor': 504,\n",
       " 'jail': 505,\n",
       " 'having': 506,\n",
       " 'permit': 507,\n",
       " 'mozambique': 508,\n",
       " 'begin': 509,\n",
       " 'because': 510,\n",
       " 'killer': 511,\n",
       " 'trial': 512,\n",
       " 'sexual': 513,\n",
       " 'relationship': 514,\n",
       " 'africa': 515,\n",
       " 'host': 516,\n",
       " 'miss': 517,\n",
       " 'pageant': 518,\n",
       " 'contest': 519,\n",
       " 'low': 520,\n",
       " 'yemen': 521,\n",
       " 'piracy': 522,\n",
       " 'ossetia': 523,\n",
       " 'abkhazia': 524,\n",
       " 'african': 525,\n",
       " 'deferred': 526,\n",
       " 'finally': 527,\n",
       " 'arrest': 528,\n",
       " 'mladic': 529,\n",
       " 'canada': 530,\n",
       " 'afghanistan': 531,\n",
       " 'pullout': 532,\n",
       " 'set': 533,\n",
       " 'urges': 534,\n",
       " 'announces': 535,\n",
       " 'skorean': 536,\n",
       " 'opens': 537,\n",
       " 'pct': 538,\n",
       " 'rebels': 539,\n",
       " 'rise': 540,\n",
       " 'lying': 541,\n",
       " 'smear': 542,\n",
       " 'dlrs': 543,\n",
       " 'calls': 544,\n",
       " 'rejects': 545,\n",
       " 'girds': 546,\n",
       " 'worst': 547,\n",
       " 'closes': 548,\n",
       " 'hint': 549,\n",
       " 'struggles': 550,\n",
       " 'focus': 551,\n",
       " 'pm': 552,\n",
       " 'denies': 553,\n",
       " 'hopes': 554,\n",
       " 'affair': 555,\n",
       " 'australia': 556,\n",
       " 'shrunk': 557,\n",
       " 'record': 558,\n",
       " 'june': 559,\n",
       " 'quarter': 560,\n",
       " 'soaring': 561,\n",
       " 'figures': 562,\n",
       " 'passenger': 563,\n",
       " 'bus': 564,\n",
       " 'strife': 565,\n",
       " 'torn': 566,\n",
       " 'kim': 567,\n",
       " 'sook': 568,\n",
       " 'work': 569,\n",
       " 'disable': 570,\n",
       " 'plants': 571,\n",
       " 'stop': 572,\n",
       " 'typical': 573,\n",
       " 'brinkmanship': 574,\n",
       " 'sweeping': 575,\n",
       " 'including': 576,\n",
       " 'income': 577,\n",
       " 'corporate': 578,\n",
       " 'boost': 579,\n",
       " 'stimulating': 580,\n",
       " 'sluggish': 581,\n",
       " 'private': 582,\n",
       " 'consumption': 583,\n",
       " 'property': 584,\n",
       " 'struggling': 585,\n",
       " 'huge': 586,\n",
       " 'debts': 587,\n",
       " 'blamed': 588,\n",
       " 'asset': 589,\n",
       " 'depreciation': 590,\n",
       " 'libyan': 591,\n",
       " 'moamer': 592,\n",
       " 'promised': 593,\n",
       " 'see': 594,\n",
       " 'ministries': 595,\n",
       " 'dismantled': 596,\n",
       " 'revenues': 597,\n",
       " 'going': 598,\n",
       " 'directly': 599,\n",
       " 'pockets': 600,\n",
       " 'humanitarian': 601,\n",
       " 'holmes': 602,\n",
       " 'tour': 603,\n",
       " 'affected': 604,\n",
       " 'left': 605,\n",
       " 'eight': 606,\n",
       " 'million': 607,\n",
       " 'need': 608,\n",
       " 'urgent': 609,\n",
       " 'food': 610,\n",
       " 'subdued': 611,\n",
       " 'trading': 612,\n",
       " 'ahead': 613,\n",
       " 'enjoying': 614,\n",
       " 'quality': 615,\n",
       " 'thanks': 616,\n",
       " 'taken': 617,\n",
       " 'olympics': 618,\n",
       " 'locals': 619,\n",
       " 'anti': 620,\n",
       " 'pollution': 621,\n",
       " 'be': 622,\n",
       " 'permanent': 623,\n",
       " 'development': 624,\n",
       " 'refused': 625,\n",
       " 'comment': 626,\n",
       " 'report': 627,\n",
       " 'still': 628,\n",
       " 'seeking': 629,\n",
       " 'stake': 630,\n",
       " 'slowed': 631,\n",
       " 'signs': 632,\n",
       " 'stabilising': 633,\n",
       " 'statistical': 634,\n",
       " 'office': 635,\n",
       " 'regular': 636,\n",
       " 'months': 637,\n",
       " 'conservative': 638,\n",
       " 'eager': 639,\n",
       " 'pro': 640,\n",
       " 'agenda': 641,\n",
       " 'rule': 642,\n",
       " 'liberal': 643,\n",
       " 'presidents': 644,\n",
       " 'american': 645,\n",
       " 'been': 646,\n",
       " 'arrested': 647,\n",
       " 'committing': 648,\n",
       " 'indecent': 649,\n",
       " 'acts': 650,\n",
       " 'old': 651,\n",
       " 'prostitute': 652,\n",
       " 'dark': 653,\n",
       " 'algerian': 654,\n",
       " 'cabinet': 655,\n",
       " 'chaired': 656,\n",
       " 'abdelaziz': 657,\n",
       " 'bouteflika': 658,\n",
       " 'sunday': 659,\n",
       " 'adopted': 660,\n",
       " 'predicated': 661,\n",
       " 'wounded': 662,\n",
       " 'station': 663,\n",
       " 'caucasian': 664,\n",
       " 'republic': 665,\n",
       " 'bordering': 666,\n",
       " 'chechnya': 667,\n",
       " 'interfax': 668,\n",
       " 'pursued': 669,\n",
       " 'limited': 670,\n",
       " 'murder': 671,\n",
       " 'hunting': 672,\n",
       " 'insurgents': 673,\n",
       " 'large': 674,\n",
       " 'islamic': 675,\n",
       " 'minority': 676,\n",
       " 'observed': 677,\n",
       " 'holy': 678,\n",
       " 'fall': 679,\n",
       " 'took': 680,\n",
       " 'profit': 681,\n",
       " 'gains': 682,\n",
       " 'measuring': 683,\n",
       " 'richter': 684,\n",
       " 'scale': 685,\n",
       " 'rattled': 686,\n",
       " 'reports': 687,\n",
       " 'casualties': 688,\n",
       " 'significant': 689,\n",
       " 'damage': 690,\n",
       " 'scientists': 691,\n",
       " 'rose': 692,\n",
       " 'less': 693,\n",
       " 'shutdown': 694,\n",
       " 'production': 695,\n",
       " 'hundreds': 696,\n",
       " 'holidaymakers': 697,\n",
       " 'thai': 698,\n",
       " 'head': 699,\n",
       " 'budget': 700,\n",
       " 'airline': 701,\n",
       " 'jetstar': 702,\n",
       " 'send': 703,\n",
       " 'chartered': 704,\n",
       " 'plane': 705,\n",
       " 'collect': 706,\n",
       " 'them': 707,\n",
       " 'swelled': 708,\n",
       " 'mainly': 709,\n",
       " 'raw': 710,\n",
       " 'material': 711,\n",
       " 'ounce': 712,\n",
       " 'bloc': 713,\n",
       " 'moscow': 714,\n",
       " 'scrutiny': 715,\n",
       " 'although': 716,\n",
       " 'sanctions': 717,\n",
       " 'cards': 718,\n",
       " 'mauritania': 719,\n",
       " 'coup': 720,\n",
       " 'd': 721,\n",
       " 'etat': 722,\n",
       " 'television': 723,\n",
       " 'financial': 724,\n",
       " 'public': 725,\n",
       " 'energy': 726,\n",
       " 'firm': 727,\n",
       " 'uk': 728,\n",
       " 'owned': 729,\n",
       " 'german': 730,\n",
       " 'apologised': 731,\n",
       " 'senior': 732,\n",
       " 'executives': 733,\n",
       " 'joked': 734,\n",
       " 'harsh': 735,\n",
       " 'mean': 736,\n",
       " 'money': 737,\n",
       " 'roman': 738,\n",
       " 'workplaces': 739,\n",
       " 'break': 740,\n",
       " 'families': 741,\n",
       " 'disrupt': 742,\n",
       " 'communities': 743,\n",
       " 'addressing': 744,\n",
       " 'flawed': 745,\n",
       " 'system': 746,\n",
       " 'countries': 747,\n",
       " 'turn': 748,\n",
       " 'blind': 749,\n",
       " 'eye': 750,\n",
       " 'fraud': 751,\n",
       " 'legislative': 752,\n",
       " 'recognize': 753,\n",
       " 'outgoing': 754,\n",
       " 'authoritarian': 755,\n",
       " 'regime': 756,\n",
       " 'whose': 757,\n",
       " 'mathematical': 758,\n",
       " 'model': 759,\n",
       " 'correctly': 760,\n",
       " 'predicted': 761,\n",
       " 'every': 762,\n",
       " 'winner': 763,\n",
       " 'vote': 764,\n",
       " 'banking': 765,\n",
       " 'victory': 766,\n",
       " 'november': 767,\n",
       " 'level': 768,\n",
       " 'tensions': 769,\n",
       " 'massive': 770,\n",
       " 'export': 771,\n",
       " 'surplus': 772,\n",
       " 'contributing': 773,\n",
       " 'divisions': 774,\n",
       " 'sought': 775,\n",
       " 'safer': 776,\n",
       " 'haven': 777,\n",
       " 'worries': 778,\n",
       " 'act': 779,\n",
       " 'sense': 780,\n",
       " 'urgency': 781,\n",
       " 'protect': 782,\n",
       " 'rainforests': 783,\n",
       " 'warning': 784,\n",
       " 'connected': 785,\n",
       " 'phenomenon': 786,\n",
       " 'climate': 787,\n",
       " 'change': 788,\n",
       " 'look': 789,\n",
       " 'became': 790,\n",
       " 'mandatory': 791,\n",
       " 'shift': 792,\n",
       " 'what': 793,\n",
       " 'social': 794,\n",
       " 'networking': 795,\n",
       " 'website': 796,\n",
       " 'faster': 797,\n",
       " 'streamlined': 798,\n",
       " 'format': 799,\n",
       " 'rebellion': 800,\n",
       " 'rejected': 801,\n",
       " 'coastal': 802,\n",
       " 'ravaging': 803,\n",
       " 'cuba': 804,\n",
       " 'caribbean': 805,\n",
       " 'faced': 806,\n",
       " 'rapidly': 807,\n",
       " 'interest': 808,\n",
       " 'percentage': 809,\n",
       " 'point': 810,\n",
       " 'double': 811,\n",
       " 'reduction': 812,\n",
       " 'forecast': 813,\n",
       " 'ibrahim': 814,\n",
       " 'ag': 815,\n",
       " 'bahanga': 816,\n",
       " 'held': 817,\n",
       " 'renegade': 818,\n",
       " 'important': 819,\n",
       " 'step': 820,\n",
       " 'forward': 821,\n",
       " 'peace': 822,\n",
       " 'organization': 823,\n",
       " 'europe': 824,\n",
       " 'body': 825,\n",
       " 'intensify': 826,\n",
       " 'sergei': 827,\n",
       " 'polish': 828,\n",
       " 'capital': 829,\n",
       " 'warsaw': 830,\n",
       " 'conflict': 831,\n",
       " 'erupted': 832,\n",
       " 'just': 833,\n",
       " 'ago': 834,\n",
       " 'york': 835,\n",
       " 'michael': 836,\n",
       " 'bloomberg': 837,\n",
       " 'lambasted': 838,\n",
       " 'rebuilding': 839,\n",
       " 'site': 840,\n",
       " 'terrorist': 841,\n",
       " 'ailing': 842,\n",
       " 'key': 843,\n",
       " 'posted': 844,\n",
       " 'linked': 845,\n",
       " 'subprime': 846,\n",
       " 'real': 847,\n",
       " 'estate': 848,\n",
       " 'crisis': 849,\n",
       " 'seven': 850,\n",
       " 'men': 851,\n",
       " 'plotting': 852,\n",
       " 'blow': 853,\n",
       " 'airliners': 854,\n",
       " 'atlantic': 855,\n",
       " 'ocean': 856,\n",
       " 'face': 857,\n",
       " 'prosecutors': 858,\n",
       " 'jury': 859,\n",
       " 'london': 860,\n",
       " 'failed': 861,\n",
       " 'verdicts': 862,\n",
       " 'shot': 863,\n",
       " 'position': 864,\n",
       " 'violation': 865,\n",
       " 'fragile': 866,\n",
       " 'brokered': 867,\n",
       " 'ceasefire': 868,\n",
       " 'remit': 869,\n",
       " 'observers': 870,\n",
       " 'gordon': 871,\n",
       " 'brown': 872,\n",
       " 'favoring': 873,\n",
       " 'either': 874,\n",
       " 'despite': 875,\n",
       " 'writing': 876,\n",
       " 'back': 877,\n",
       " 'democrats': 878,\n",
       " 'have': 879,\n",
       " 'mounted': 880,\n",
       " 'hour': 881,\n",
       " 'patrol': 882,\n",
       " 'village': 883,\n",
       " 'where': 884,\n",
       " 'lives': 885,\n",
       " 'robbers': 886,\n",
       " 'botched': 887,\n",
       " 'attempt': 888,\n",
       " 'rob': 889,\n",
       " 'her': 890,\n",
       " 'solar': 891,\n",
       " 'panel': 892,\n",
       " 'holds': 893,\n",
       " 'presidency': 894,\n",
       " 'joint': 895,\n",
       " 'approach': 896,\n",
       " 'resolving': 897,\n",
       " 'region': 898,\n",
       " 'front': 899,\n",
       " 'lines': 900,\n",
       " 'warming': 901,\n",
       " 'villages': 902,\n",
       " 'packing': 903,\n",
       " 'leave': 904,\n",
       " 'rolling': 905,\n",
       " 'barbed': 906,\n",
       " 'wire': 907,\n",
       " 'removing': 908,\n",
       " 'equipment': 909,\n",
       " 'process': 910,\n",
       " 'anything': 911,\n",
       " 'fast': 912,\n",
       " 'residents': 913,\n",
       " 'faith': 914,\n",
       " 'foil': 915,\n",
       " 'even': 916,\n",
       " 'if': 917,\n",
       " 'you': 918,\n",
       " 're': 919,\n",
       " 'flogging': 920,\n",
       " 'god': 921,\n",
       " 'bibles': 922,\n",
       " 'finding': 923,\n",
       " 'between': 924,\n",
       " 'rival': 925,\n",
       " 'english': 926,\n",
       " 'domestic': 927,\n",
       " 'giants': 928,\n",
       " 'rights': 929,\n",
       " 'lucrative': 930,\n",
       " 'iron': 931,\n",
       " 'ore': 932,\n",
       " 'deposits': 933,\n",
       " 'intervene': 934,\n",
       " 'second': 935,\n",
       " 'participants': 936,\n",
       " 'hopefully': 937,\n",
       " 'accord': 938,\n",
       " 'jean': 939,\n",
       " 'son': 940,\n",
       " 'nicolas': 941,\n",
       " 'exchanged': 942,\n",
       " 'wedding': 943,\n",
       " 'vows': 944,\n",
       " 'heir': 945,\n",
       " 'retailing': 946,\n",
       " 'fortune': 947,\n",
       " 'father': 948,\n",
       " 'paris': 949,\n",
       " 'turkish': 950,\n",
       " 'abdullah': 951,\n",
       " 'azerbaijani': 952,\n",
       " 'counterpart': 953,\n",
       " 'ilham': 954,\n",
       " 'aliyev': 955,\n",
       " 'stressed': 956,\n",
       " 'friendly': 957,\n",
       " 'arch': 958,\n",
       " 'enemy': 959,\n",
       " 'crew': 960,\n",
       " 'crashed': 961,\n",
       " 'near': 962,\n",
       " 'town': 963,\n",
       " 'afula': 964,\n",
       " 'burst': 965,\n",
       " 'flames': 966,\n",
       " 'radio': 967,\n",
       " 'alliance': 968,\n",
       " 'their': 969,\n",
       " 'george': 970,\n",
       " 'w': 971,\n",
       " 'bush': 972,\n",
       " 'ordering': 973,\n",
       " 'machinists': 974,\n",
       " 'entered': 975,\n",
       " 'sign': 976,\n",
       " 'resumption': 977,\n",
       " 'contract': 978,\n",
       " 'horizon': 979,\n",
       " 'sides': 980,\n",
       " 'miami': 981,\n",
       " 'based': 982,\n",
       " 'web': 983,\n",
       " 'soared': 984,\n",
       " 'use': 985,\n",
       " 'become': 986,\n",
       " 'commonplace': 987,\n",
       " 'presenting': 988,\n",
       " 'far': 989,\n",
       " 'dangers': 990,\n",
       " 'traditional': 991,\n",
       " 'conference': 992,\n",
       " 'stockholm': 993,\n",
       " 'wrapped': 994,\n",
       " 'fire': 995,\n",
       " 'during': 996,\n",
       " 'city': 997,\n",
       " 'nablus': 998,\n",
       " 'canceled': 999,\n",
       " ...}"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_vocab_size = 2 ** 10\n",
    "def build_vocabulary(texts,max_vocab_size):\n",
    "    \n",
    "    word_dict = dict()\n",
    "    word_counts = collections.Counter(' '.join(texts).split(' ')).most_common()\n",
    "    for word, _ in word_counts:\n",
    "        word_dict[word] = len(word_dict)\n",
    "                            \n",
    "    reversed_dict = dict(zip(word_dict.values(), word_dict.keys()))\n",
    "    \n",
    "    return word_dict,reversed_dict\n",
    "    \n",
    "word_dict,reversed_dict = build_vocabulary(article + title,max_vocab_size)\n",
    "vocab_size = len(word_dict)\n",
    "print ('The cocabulary size of word dict is: {}'.format(vocab_size))\n",
    "word_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in d:\\anaconda3.5.20\\envs\\tensorflow2.0\\lib\\site-packages\n",
      "Requirement already satisfied: six in d:\\anaconda3.5.20\\envs\\tensorflow2.0\\lib\\site-packages (from nltk)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using pip version 9.0.1, however version 19.2.3 is available.\n",
      "You should consider upgrading via the 'python -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# 分词\n",
    "def tokensize_text(text):\n",
    "    words = []\n",
    "    for w in word_tokenize(process_text(text)):\n",
    "        words.append(w)\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['australia',\n",
       " 's',\n",
       " 'current',\n",
       " 'account',\n",
       " 'deficit',\n",
       " 'shrunk',\n",
       " 'by',\n",
       " 'a',\n",
       " 'record',\n",
       " '.',\n",
       " 'billion',\n",
       " 'dollars',\n",
       " 'lrb',\n",
       " '.',\n",
       " 'billion',\n",
       " 'us',\n",
       " 'rrb',\n",
       " 'in',\n",
       " 'the',\n",
       " 'june',\n",
       " 'quarter',\n",
       " 'due',\n",
       " 'to',\n",
       " 'soaring',\n",
       " 'commodity',\n",
       " 'prices',\n",
       " ',',\n",
       " 'figures',\n",
       " 'released',\n",
       " 'monday',\n",
       " 'showed',\n",
       " '.']"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "artile_words = tokensize_text(article[0])\n",
    "artile_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "def words2ids(text,text_dict):\n",
    "    ids = []\n",
    "    words = tokensize_text(text)\n",
    "    for w in words:\n",
    "        ids.append(text_dict[w])\n",
    "        \n",
    "    return ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[556,\n",
       " 10,\n",
       " 292,\n",
       " 293,\n",
       " 200,\n",
       " 557,\n",
       " 17,\n",
       " 4,\n",
       " 558,\n",
       " 1,\n",
       " 83,\n",
       " 39,\n",
       " 201,\n",
       " 1,\n",
       " 83,\n",
       " 12,\n",
       " 202,\n",
       " 3,\n",
       " 0,\n",
       " 559,\n",
       " 560,\n",
       " 67,\n",
       " 6,\n",
       " 561,\n",
       " 139,\n",
       " 18,\n",
       " 2,\n",
       " 562,\n",
       " 294,\n",
       " 13,\n",
       " 295,\n",
       " 1]"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article_ids = words2ids(article[0],word_dict)\n",
    "article_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ids2words(ids,reversed_dict):\n",
    "    words = []\n",
    "    for id in ids:\n",
    "        words.append(reversed_dict[id])\n",
    "     \n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['australia',\n",
       " 's',\n",
       " 'current',\n",
       " 'account',\n",
       " 'deficit',\n",
       " 'shrunk',\n",
       " 'by',\n",
       " 'a',\n",
       " 'record',\n",
       " '.',\n",
       " 'billion',\n",
       " 'dollars',\n",
       " 'lrb',\n",
       " '.',\n",
       " 'billion',\n",
       " 'us',\n",
       " 'rrb',\n",
       " 'in',\n",
       " 'the',\n",
       " 'june',\n",
       " 'quarter',\n",
       " 'due',\n",
       " 'to',\n",
       " 'soaring',\n",
       " 'commodity',\n",
       " 'prices',\n",
       " ',',\n",
       " 'figures',\n",
       " 'released',\n",
       " 'monday',\n",
       " 'showed',\n",
       " '.']"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article_words = ids2words(article_ids,reversed_dict)\n",
    "article_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_data_int(text,word_dict):\n",
    "    dataset = []\n",
    "    for i in range(len(text)):\n",
    "        line_indices = words2ids(text[i],word_dict)\n",
    "        print (line_indices)\n",
    "        dataset.append(line_indices)\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[556, 10, 292, 293, 200, 557, 17, 4, 558, 1, 83, 39, 201, 1, 83, 12, 202, 3, 0, 559, 560, 67, 6, 561, 139, 18, 2, 562, 294, 13, 295, 1]\n",
      "[22, 203, 43, 84, 55, 103, 3, 4, 296, 297, 204, 7, 4, 563, 564, 3, 0, 565, 566, 40, 205, 7, 13, 2, 0, 104, 11, 1]\n",
      "[85, 31, 44, 24, 1, 20, 13, 105, 4, 298, 299, 27, 0, 140, 141, 9, 50, 139, 18, 2, 45, 11, 1]\n",
      "[32, 46, 10, 206, 68, 567, 568, 207, 142, 46, 13, 6, 300, 569, 6, 570, 16, 206, 571, 9, 572, 16, 573, 574, 3, 143, 1]\n",
      "[32, 46, 7, 13, 106, 575, 208, 209, 2, 576, 577, 9, 578, 208, 301, 6, 579, 302, 17, 580, 581, 582, 583, 9, 303, 107, 1]\n",
      "[144, 69, 18, 44, 24, 1, 20, 13, 7, 56, 51, 210, 9, 304, 305, 306, 27, 307, 308, 108, 309, 310, 2, 45, 11, 1]\n",
      "[85, 31, 44, 24, 1, 20, 13, 105, 4, 298, 299, 27, 0, 140, 141, 9, 50, 139, 18, 2, 45, 11, 1]\n",
      "[211, 584, 311, 312, 2, 585, 313, 586, 587, 2, 106, 70, 5, 1, 83, 314, 14, 0, 109, 315, 5, 71, 23, 588, 7, 589, 590, 1]\n",
      "[591, 145, 592, 316, 13, 593, 317, 146, 9, 318, 209, 33, 212, 11, 86, 594, 595, 596, 9, 34, 597, 598, 599, 147, 0, 600, 5, 0, 84, 1]\n",
      "[0, 140, 319, 601, 57, 148, 602, 320, 3, 321, 13, 6, 603, 322, 604, 17, 323, 2, 71, 35, 605, 149, 606, 607, 84, 3, 608, 5, 609, 610, 324, 1]\n",
      "[25, 87, 69, 18, 44, 1, 20, 88, 13, 3, 611, 612, 613, 5, 4, 12, 213, 2, 45, 11, 1]\n",
      "[325, 36, 614, 16, 326, 327, 615, 3, 4, 214, 616, 6, 328, 617, 14, 0, 618, 2, 110, 11, 13, 2, 72, 4, 329, 17, 149, 619, 14, 0, 620, 621, 150, 6, 622, 215, 623, 1]\n",
      "[32, 330, 31, 44, 1, 20, 50, 13, 105, 331, 70, 7, 56, 51, 9, 0, 216, 10, 332, 217, 21, 0, 151, 2, 152, 11, 1]\n",
      "[32, 46, 10, 47, 333, 46, 624, 73, 625, 626, 13, 7, 4, 218, 334, 627, 33, 23, 36, 628, 629, 4, 630, 3, 219, 12, 107, 73, 58, 153, 1]\n",
      "[144, 31, 44, 24, 1, 20, 13, 7, 56, 51, 210, 9, 304, 305, 306, 27, 307, 308, 108, 309, 310, 2, 45, 11, 1]\n",
      "[32, 46, 10, 335, 154, 631, 336, 3, 155, 15, 34, 9, 139, 18, 295, 632, 5, 633, 2, 0, 337, 634, 635, 11, 13, 1]\n",
      "[32, 46, 10, 25, 338, 339, 16, 109, 636, 340, 13, 37, 637, 5, 220, 2, 28, 0, 638, 111, 639, 6, 329, 341, 4, 640, 303, 641, 37, 4, 214, 5, 642, 17, 643, 644, 1]\n",
      "[19, 645, 221, 35, 646, 647, 9, 112, 3, 342, 28, 648, 649, 650, 21, 4, 113, 651, 343, 652, 7, 4, 653, 51, 2, 74, 9, 156, 157, 11, 13, 1]\n",
      "[114, 69, 18, 44, 158, 159, 13, 15, 89, 222, 0, 344, 7, 34, 18, 27, 59, 160, 15, 23, 345, 147, 0, 12, 41, 161, 2, 45, 11, 1]\n",
      "[90, 91, 69, 18, 162, 13, 115, 1, 20, 24, 2, 346, 56, 51, 210, 163, 2, 45, 11, 1]\n",
      "[0, 654, 655, 656, 17, 38, 657, 658, 7, 659, 660, 0, 347, 348, 661, 7, 19, 34, 349, 5, 39, 4, 350, 9, 4, 302, 154, 5, 1, 20, 2, 23, 48, 106, 351, 1]\n",
      "[114, 69, 18, 44, 158, 159, 13, 15, 89, 222, 0, 344, 7, 34, 18, 27, 59, 160, 15, 23, 345, 147, 0, 12, 41, 161, 2, 45, 11, 1]\n",
      "[43, 352, 55, 103, 9, 92, 48, 662, 3, 19, 204, 7, 4, 74, 663, 223, 13, 3, 52, 10, 664, 665, 5, 353, 666, 7, 667, 2, 668, 164, 354, 165, 1]\n",
      "[32, 330, 31, 116, 224, 355, 13, 115, 105, 56, 51, 10, 331, 163, 70, 9, 0, 216, 10, 332, 217, 21, 0, 151, 2, 152, 11, 1]\n",
      "[114, 117, 7, 13, 669, 670, 356, 21, 357, 225, 358, 226, 5, 671, 26, 11, 359, 55, 53, 672, 118, 0, 673, 15, 0, 674, 675, 676, 677, 0, 678, 119, 5, 360, 1]\n",
      "[90, 91, 69, 18, 55, 24, 1, 20, 7, 13, 2, 346, 56, 51, 10, 679, 7, 163, 2, 45, 11, 1]\n",
      "[361, 69, 18, 227, 24, 1, 20, 13, 2, 15, 89, 680, 681, 37, 166, 120, 10, 682, 7, 0, 362, 2, 45, 11, 1]\n",
      "[4, 363, 167, 683, 1, 7, 0, 684, 685, 686, 0, 364, 142, 121, 5, 25, 87, 13, 26, 168, 55, 75, 223, 687, 5, 688, 365, 689, 690, 2, 691, 11, 1]\n",
      "[76, 34, 18, 692, 17, 693, 228, 92, 151, 3, 366, 60, 7, 13, 37, 59, 160, 367, 0, 694, 5, 368, 118, 34, 695, 3, 0, 41, 5, 169, 2, 152, 11, 1]\n",
      "[696, 5, 85, 697, 369, 7, 0, 698, 170, 121, 5, 370, 122, 371, 6, 699, 372, 13, 37, 700, 701, 702, 11, 23, 86, 703, 4, 704, 705, 6, 706, 707, 1]\n",
      "[32, 46, 10, 60, 200, 708, 6, 1, 83, 39, 3, 155, 67, 709, 67, 6, 373, 34, 9, 710, 711, 18, 2, 0, 123, 11, 13, 1]\n",
      "[90, 91, 374, 18, 227, 50, 7, 13, 22, 1, 1, 12, 39, 19, 712, 2, 24, 27, 163, 10, 29, 5, 1, 1, 39, 1]\n",
      "[124, 229, 358, 125, 171, 0, 713, 10, 230, 28, 714, 313, 715, 22, 19, 231, 172, 7, 13, 2, 716, 717, 21, 52, 122, 53, 3, 0, 718, 1]\n",
      "[719, 10, 375, 3, 77, 173, 19, 155, 720, 721, 722, 106, 223, 13, 0, 376, 5, 4, 123, 2, 47, 723, 165, 1]\n",
      "[377, 10, 724, 378, 122, 44, 7, 13, 14, 4, 725, 213, 1]\n",
      "[93, 78, 57, 232, 233, 234, 355, 8, 235, 236, 237, 238, 61, 17, 12, 239, 240, 174, 9, 11, 0, 79, 86, 241, 16, 242, 22, 118, 243, 1]\n",
      "[726, 727, 244, 728, 2, 71, 36, 729, 17, 730, 77, 108, 244, 2, 731, 7, 8, 37, 92, 5, 16, 732, 733, 734, 33, 4, 735, 379, 86, 736, 175, 737, 14, 12, 1]\n",
      "[12, 738, 380, 381, 7, 8, 112, 33, 245, 61, 7, 12, 739, 740, 42, 741, 9, 742, 743, 2, 126, 744, 0, 79, 10, 745, 245, 746, 1]\n",
      "[246, 747, 125, 748, 4, 749, 750, 6, 751, 3, 176, 119, 10, 752, 382, 9, 753, 4, 383, 14, 0, 754, 755, 756, 2, 4, 384, 177, 111, 145, 11, 8, 1]\n",
      "[4, 385, 757, 758, 759, 35, 760, 761, 762, 763, 5, 0, 247, 248, 386, 764, 173, 36, 765, 7, 4, 387, 766, 14, 249, 127, 49, 3, 767, 1]\n",
      "[0, 247, 248, 250, 388, 25, 389, 8, 15, 127, 49, 226, 148, 128, 5, 390, 6, 391, 9, 392, 393, 3, 394, 6, 0, 249, 10, 395, 251, 5, 178, 7, 4, 396, 1]\n",
      "[0, 140, 141, 9, 397, 129, 373, 768, 60, 94, 3, 398, 176, 120, 72, 769, 30, 0, 366, 77, 10, 770, 771, 772, 9, 399, 400, 4, 401, 402, 252, 1]\n",
      "[38, 253, 254, 5, 62, 7, 8, 255, 0, 12, 63, 256, 2, 257, 258, 5, 773, 6, 774, 3, 0, 79, 71, 0, 123, 403, 48, 404, 405, 406, 259, 1]\n",
      "[0, 80, 116, 179, 1, 39, 14, 0, 109, 180, 3, 368, 4, 113, 8, 15, 89, 775, 4, 776, 777, 27, 130, 260, 3, 0, 181, 9, 34, 18, 116, 7, 407, 778, 1]\n",
      "[408, 10, 409, 410, 411, 7, 0, 76, 8, 6, 779, 28, 4, 780, 5, 412, 781, 6, 782, 0, 783, 2, 784, 359, 55, 261, 785, 6, 0, 786, 5, 787, 788, 1]\n",
      "[413, 10, 25, 789, 790, 791, 8, 3, 4, 792, 6, 793, 0, 386, 794, 795, 796, 262, 36, 4, 797, 2, 798, 9, 175, 261, 799, 33, 35, 149, 414, 3, 4, 47, 5, 800, 1]\n",
      "[0, 12, 47, 415, 11, 8, 23, 416, 417, 75, 418, 263, 27, 62, 33, 23, 48, 419, 0, 12, 63, 168, 26, 801, 0, 182, 21, 258, 15, 420, 1]\n",
      "[0, 12, 47, 415, 11, 8, 23, 416, 417, 75, 418, 263, 27, 62, 33, 23, 48, 419, 0, 12, 63, 168, 26, 11, 0, 182, 215, 21, 258, 122, 420, 1]\n",
      "[421, 38, 253, 254, 7, 8, 264, 0, 12, 63, 6, 422, 423, 424, 425, 426, 2, 257, 0, 68, 5, 427, 0, 428, 5, 0, 79, 17, 429, 430, 1]\n",
      "[81, 110, 255, 802, 431, 8, 15, 183, 59, 95, 265, 6, 4, 184, 43, 185, 3, 0, 41, 5, 169, 9, 404, 266, 0, 40, 12, 161, 37, 803, 804, 9, 0, 805, 1]\n",
      "[806, 28, 4, 807, 401, 252, 2, 25, 87, 10, 364, 73, 432, 0, 96, 808, 154, 17, 315, 4, 809, 810, 7, 186, 2, 811, 0, 812, 813, 17, 152, 1]\n",
      "[433, 225, 57, 814, 815, 816, 35, 294, 0, 166, 434, 817, 17, 131, 818, 311, 3, 0, 142, 5, 435, 2, 0, 123, 35, 11, 2, 3, 19, 819, 820, 821, 405, 822, 1]\n",
      "[421, 38, 253, 254, 7, 8, 264, 0, 12, 63, 6, 422, 423, 424, 425, 426, 2, 257, 0, 68, 5, 427, 0, 428, 5, 0, 79, 17, 429, 430, 1]\n",
      "[93, 78, 57, 232, 233, 234, 8, 235, 236, 237, 238, 61, 17, 12, 239, 240, 174, 9, 11, 0, 79, 86, 241, 16, 242, 22, 118, 243, 1]\n",
      "[0, 247, 248, 250, 388, 25, 389, 8, 15, 127, 49, 226, 148, 128, 5, 390, 6, 391, 9, 392, 393, 3, 394, 6, 0, 249, 10, 395, 251, 5, 178, 7, 4, 396, 1]\n",
      "[0, 823, 14, 97, 9, 436, 3, 824, 201, 437, 202, 2, 4, 124, 97, 825, 2, 207, 8, 16, 438, 141, 6, 826, 439, 6, 440, 24, 7, 441, 132, 1]\n",
      "[187, 442, 188, 827, 443, 320, 8, 3, 0, 828, 829, 830, 14, 131, 109, 98, 6, 19, 133, 438, 47, 173, 0, 54, 831, 832, 833, 30, 4, 119, 834, 1]\n",
      "[93, 78, 57, 232, 233, 234, 8, 235, 236, 237, 238, 61, 17, 12, 239, 240, 174, 9, 11, 0, 79, 86, 241, 16, 242, 22, 118, 243, 1]\n",
      "[0, 80, 2, 444, 17, 445, 5, 130, 3, 0, 181, 2, 116, 179, 1, 39, 14, 0, 109, 180, 3, 4, 113, 8, 1]\n",
      "[143, 7, 77, 189, 3, 64, 162, 8, 126, 4, 65, 2, 26, 134, 38, 267, 99, 9, 177, 145, 446, 190, 447, 33, 448, 36, 449, 29, 1]\n",
      "[25, 835, 450, 836, 837, 7, 8, 838, 220, 3, 839, 22, 0, 840, 5, 0, 841, 268, 21, 0, 76, 60, 451, 1]\n",
      "[842, 107, 73, 58, 153, 106, 269, 8, 6, 191, 270, 843, 192, 6, 271, 42, 16, 272, 15, 23, 844, 175, 273, 70, 845, 6, 0, 12, 846, 847, 848, 849, 1]\n",
      "[4, 66, 167, 274, 40, 100, 7, 8, 2, 275, 452, 84, 2, 9, 276, 277, 5, 278, 7, 4, 170, 121, 3, 279, 41, 280, 1]\n",
      "[12, 82, 453, 88, 8, 15, 89, 454, 219, 107, 108, 58, 153, 10, 164, 5, 273, 70, 9, 269, 6, 191, 192, 6, 271, 42, 16, 272, 1]\n",
      "[850, 851, 296, 5, 852, 6, 853, 42, 854, 30, 0, 855, 856, 3, 857, 4, 455, 2, 858, 11, 8, 2, 37, 4, 859, 3, 860, 861, 6, 456, 862, 1]\n",
      "[54, 11, 92, 5, 16, 352, 48, 863, 135, 27, 4, 187, 864, 8, 3, 865, 5, 4, 866, 457, 2, 15, 4, 384, 440, 458, 3, 0, 133, 867, 868, 30, 0, 869, 5, 133, 870, 1]\n",
      "[218, 459, 188, 871, 872, 460, 8, 873, 874, 127, 49, 365, 148, 128, 3, 0, 12, 193, 250, 2, 875, 876, 19, 461, 3, 71, 212, 458, 6, 877, 0, 878, 1]\n",
      "[462, 74, 879, 4, 880, 4, 881, 882, 3, 4, 883, 884, 12, 193, 281, 127, 49, 10, 463, 885, 37, 886, 215, 4, 887, 888, 6, 889, 890, 891, 892, 2, 19, 96, 11, 1]\n",
      "[464, 2, 71, 893, 0, 124, 229, 894, 2, 411, 8, 14, 4, 895, 124, 896, 6, 897, 0, 465, 3, 0, 466, 2, 4, 898, 7, 0, 899, 900, 5, 402, 901, 1]\n",
      "[12, 82, 453, 88, 8, 15, 89, 454, 219, 107, 108, 58, 153, 10, 164, 5, 273, 70, 9, 269, 6, 191, 192, 6, 271, 42, 16, 272, 1]\n",
      "[143, 7, 77, 189, 3, 64, 162, 8, 126, 4, 65, 2, 26, 134, 38, 267, 99, 9, 177, 145, 446, 190, 447, 33, 448, 36, 449, 29, 1]\n",
      "[3, 902, 3, 246, 54, 2, 187, 467, 122, 903, 42, 6, 904, 2, 905, 42, 906, 907, 9, 908, 909, 2, 26, 0, 910, 36, 911, 26, 912, 2, 913, 11, 8, 1]\n",
      "[914, 36, 75, 915, 21, 362, 174, 53, 916, 917, 918, 919, 920, 0, 263, 5, 921, 2, 15, 0, 282, 468, 5, 922, 3, 0, 76, 36, 923, 469, 1]\n",
      "[4, 470, 924, 925, 926, 9, 927, 471, 928, 30, 929, 6, 930, 931, 932, 933, 35, 367, 472, 74, 6, 934, 3, 473, 474, 2, 157, 11, 8, 1]\n",
      "[0, 935, 136, 5, 143, 7, 77, 189, 3, 64, 162, 8, 126, 4, 65, 2, 26, 38, 267, 99, 11, 936, 125, 937, 456, 19, 938, 186, 1]\n",
      "[939, 283, 2, 475, 940, 5, 476, 38, 941, 283, 2, 942, 943, 944, 8, 28, 19, 945, 6, 4, 476, 946, 947, 3, 131, 948, 10, 146, 477, 2, 194, 5, 949, 1]\n",
      "[950, 38, 951, 478, 9, 131, 952, 953, 954, 955, 7, 8, 956, 957, 230, 105, 478, 10, 98, 6, 479, 10, 958, 959, 480, 166, 120, 1]\n",
      "[19, 481, 104, 482, 28, 4, 43, 221, 960, 961, 962, 0, 483, 963, 5, 964, 9, 965, 147, 966, 7, 8, 2, 78, 967, 11, 1]\n",
      "[52, 10, 68, 6, 484, 207, 319, 3, 0, 104, 968, 7, 8, 6, 485, 4, 98, 6, 54, 17, 969, 486, 284, 14, 176, 120, 1]\n",
      "[38, 970, 971, 1, 972, 264, 19, 231, 3, 81, 9, 47, 110, 339, 973, 431, 15, 183, 59, 95, 2, 71, 265, 8, 6, 4, 66, 184, 43, 185, 2, 487, 3, 0, 41, 5, 169, 266, 0, 12, 47, 1]\n",
      "[0, 974, 229, 488, 21, 489, 975, 4, 490, 136, 8, 28, 75, 976, 5, 4, 977, 5, 978, 94, 7, 0, 979, 2, 134, 980, 11, 1]\n",
      "[59, 95, 265, 6, 4, 66, 184, 43, 185, 8, 15, 23, 487, 341, 0, 41, 5, 169, 266, 0, 12, 47, 5, 81, 2, 157, 22, 0, 981, 982, 337, 59, 451, 11, 1]\n",
      "[491, 132, 7, 0, 983, 35, 984, 15, 492, 985, 35, 986, 987, 2, 988, 989, 175, 465, 9, 990, 228, 991, 132, 2, 493, 403, 22, 4, 992, 3, 993, 33, 994, 42, 8, 1]\n",
      "[481, 117, 227, 995, 9, 103, 4, 494, 996, 4, 495, 3, 0, 483, 194, 73, 997, 5, 998, 7, 8, 2, 97, 157, 11, 1]\n",
      "[4, 66, 167, 274, 40, 100, 7, 8, 2, 275, 137, 84, 2, 9, 276, 277, 5, 278, 7, 4, 170, 121, 3, 279, 41, 280, 1]\n",
      "[0, 496, 999, 4, 83, 151, 1000, 14, 4, 25, 1001, 1002, 497, 8, 2, 1003, 0, 1004, 112, 1005, 14, 92, 5, 16, 282, 104, 1006, 6, 4, 25, 1007, 1]\n",
      "[4, 211, 1008, 1009, 7, 8, 1010, 4, 101, 2, 314, 201, 2, 39, 202, 14, 1011, 4, 285, 498, 6, 499, 500, 501, 212, 103, 4, 1012, 1013, 1]\n",
      "[195, 35, 1014, 502, 22, 203, 43, 268, 21, 16, 1015, 503, 28, 0, 1016, 5, 442, 97, 1017, 2, 1018, 188, 1019, 1020, 11, 7, 8, 1]\n",
      "[4, 156, 1021, 19, 1022, 334, 504, 6, 92, 113, 3, 505, 7, 8, 14, 506, 1023, 131, 1024, 126, 4, 507, 27, 0, 1025, 1026, 2, 0, 47, 333, 1027, 164, 354, 165, 1]\n",
      "[508, 10, 286, 1028, 111, 125, 509, 137, 136, 94, 6, 1029, 16, 281, 14, 193, 1030, 7, 186, 2, 4, 111, 96, 11, 1]\n",
      "[4, 284, 81, 196, 48, 171, 7, 129, 8, 26, 53, 510, 0, 101, 9, 102, 3, 0, 285, 511, 10, 512, 55, 3, 4, 513, 514, 22, 0, 180, 2, 110, 11, 1]\n",
      "[32, 515, 35, 216, 16, 1031, 6, 516, 0, 517, 76, 1032, 518, 37, 0, 519, 48, 1033, 27, 1034, 30, 1035, 399, 3, 1036, 54, 2, 19, 96, 11, 8, 1]\n",
      "[0, 80, 2, 444, 17, 445, 5, 130, 3, 0, 181, 2, 116, 6, 19, 119, 520, 5, 1, 39, 3, 1037, 60, 8, 2, 16, 1038, 1039, 173, 1040, 2, 1]\n",
      "[4, 66, 167, 274, 40, 100, 7, 8, 2, 275, 137, 84, 2, 9, 276, 277, 5, 278, 7, 4, 170, 121, 3, 279, 41, 280, 1]\n",
      "[4, 284, 81, 196, 48, 171, 7, 129, 8, 26, 53, 510, 0, 101, 9, 102, 3, 0, 285, 511, 10, 512, 55, 3, 4, 513, 514, 22, 0, 180, 2, 110, 11, 1]\n",
      "[521, 35, 1041, 42, 150, 6, 1042, 522, 270, 16, 161, 2, 1043, 117, 3, 0, 41, 5, 1044, 9, 1045, 1046, 1047, 6, 287, 1048, 1049, 2, 96, 1050, 11, 7, 8, 1]\n",
      "[52, 7, 8, 460, 1051, 501, 0, 138, 10, 1052, 156, 33, 23, 1053, 1054, 1055, 30, 0, 1056, 225, 322, 5, 32, 523, 9, 524, 1]\n",
      "[4, 40, 525, 97, 172, 67, 6, 509, 8, 48, 526, 17, 4, 136, 72, 1057, 33, 64, 10, 146, 1058, 1059, 527, 1060, 4, 1061, 222, 77, 189, 65, 1]\n",
      "[138, 102, 1062, 1063, 1064, 1065, 1066, 351, 8, 400, 1067, 10, 439, 6, 528, 1068, 1069, 259, 1070, 1071, 1072, 529, 7, 4, 98, 1073, 14, 1074, 10, 133, 1075, 1]\n",
      "[0, 1076, 5, 530, 10, 117, 3, 531, 125, 53, 499, 168, 1077, 4, 532, 1078, 533, 1079, 1080, 113, 2, 459, 188, 1081, 1082, 11, 8, 1]\n",
      "[85, 292, 293, 200, 1083, 224]\n",
      "[22, 203, 43, 135, 3, 40, 205, 1084]\n",
      "[85, 82, 29, 24, 1, 20]\n",
      "[68, 534, 142, 46, 6, 300, 206, 1085]\n",
      "[197, 535, 208, 301, 6, 1086, 252]\n",
      "[144, 31, 29, 24, 1, 20]\n",
      "[85, 82, 29, 24, 1, 20]\n",
      "[1087, 10, 312, 1088, 1, 83, 80, 1089]\n",
      "[316, 1090, 317, 146, 318, 209]\n",
      "[138, 10, 1091, 324, 96, 1092, 3, 323, 1093, 321]\n",
      "[25, 87, 82, 29, 1, 20, 88]\n",
      "[325, 10, 326, 327, 3, 4, 214, 67, 6, 1094, 150]\n",
      "[536, 82, 29, 1, 20, 50]\n",
      "[197, 73, 1095, 6, 1096, 165, 94, 28, 58]\n",
      "[144, 31, 29, 24, 1, 20]\n",
      "[197, 10, 335, 154, 1097, 336, 3, 155]\n",
      "[32, 46, 10, 25, 338, 527, 537]\n",
      "[12, 221, 112, 28, 343, 1098, 3, 342]\n",
      "[114, 31, 29, 158, 159]\n",
      "[90, 91, 31, 1099, 115, 1, 538, 24]\n",
      "[1100, 1101, 347, 348, 28, 34, 171, 22, 39, 4, 350]\n",
      "[114, 31, 29, 158, 159]\n",
      "[43, 74, 1102, 103, 3, 353, 204]\n",
      "[536, 82, 224, 50, 3, 115, 340]\n",
      "[205, 1103, 356, 21, 357, 539, 15, 360, 1104]\n",
      "[90, 91, 31, 1105, 1, 538, 50]\n",
      "[361, 31, 50, 3, 115, 60]\n",
      "[363, 198, 1106, 25, 87]\n",
      "[34, 18, 540, 15, 160, 1107, 41, 1108]\n",
      "[1109, 1110, 6, 1111, 372, 1112, 369, 3, 370]\n",
      "[197, 60, 1113, 1114, 7, 34, 349, 540]\n",
      "[90, 91, 374, 537, 50]\n",
      "[133, 6, 1115, 230, 28, 52, 22, 231, 172]\n",
      "[1116, 375, 535, 376, 5, 4, 123]\n",
      "[377, 378, 44, 14, 213]\n",
      "[93, 78, 57, 288, 12, 61]\n",
      "[244, 262, 1117, 14, 1118, 30, 1119, 5, 1120, 379]\n",
      "[12, 380, 381, 1121, 12, 245, 61]\n",
      "[1122, 177, 199, 194, 30, 1123, 382]\n",
      "[261, 385, 1124, 387, 49, 383]\n",
      "[49, 199, 128, 5, 541, 30, 178, 542]\n",
      "[12, 397, 60, 94, 533, 14, 176, 120, 3, 398]\n",
      "[62, 289, 12, 63, 469, 1125, 5, 406, 259]\n",
      "[80, 290, 179, 1, 543, 7, 130, 260]\n",
      "[409, 410, 544, 14, 412, 1126, 21, 1127]\n",
      "[413, 1128, 1129, 149, 414]\n",
      "[12, 545, 182, 21, 16, 63, 3, 62]\n",
      "[12, 545, 182, 21, 16, 63, 3, 62]\n",
      "[62, 38, 289, 12, 68, 256]\n",
      "[81, 546, 14, 0, 547, 15, 183, 95, 548, 3]\n",
      "[25, 87, 1130, 432, 1131, 228, 371]\n",
      "[433, 539, 3, 435, 500, 166, 434]\n",
      "[62, 38, 289, 12, 68, 256]\n",
      "[93, 78, 57, 288, 12, 61]\n",
      "[49, 199, 128, 5, 541, 30, 178, 542]\n",
      "[437, 544, 14, 436, 3, 287, 21, 441, 132]\n",
      "[52, 10, 443, 3, 1132, 6, 1133, 12, 1134, 1135, 54]\n",
      "[93, 78, 57, 288, 12, 61]\n",
      "[80, 290, 179, 1, 543, 7, 181, 130, 260]\n",
      "[134, 99, 9, 190, 549, 64, 65, 36, 29]\n",
      "[1136, 450, 1137, 1138, 1139, 220]\n",
      "[58, 1140, 6, 191, 270, 192, 72, 1141, 70]\n",
      "[452, 135, 15, 66, 198, 291, 40, 100]\n",
      "[56, 51, 550, 88, 58, 3, 551]\n",
      "[455, 255, 3, 408, 30, 1142, 1143, 297, 1144]\n",
      "[54, 199, 52, 5, 1145, 457]\n",
      "[218, 552, 553, 1146, 49, 3, 1147, 461]\n",
      "[462, 74, 1148, 49, 10, 463, 97, 37, 1149]\n",
      "[464, 1150, 124, 1151, 1152, 14, 466, 1153]\n",
      "[56, 51, 550, 88, 58, 3, 551]\n",
      "[134, 99, 9, 190, 549, 64, 65, 36, 29]\n",
      "[187, 467, 1154, 1155, 3, 246, 54]\n",
      "[76, 10, 282, 1156, 468, 1157, 217, 3, 407]\n",
      "[472, 74, 1158, 3, 72, 473, 474, 471, 470]\n",
      "[75, 65, 1159, 26, 99, 554, 14, 64, 65, 186]\n",
      "[475, 283, 1160, 3, 1161, 10, 477]\n",
      "[1162, 479, 251, 42, 1163, 37, 480, 98]\n",
      "[104, 482, 1164, 3, 195]\n",
      "[52, 534, 484, 486, 6, 485, 54, 98]\n",
      "[81, 546, 14, 0, 547, 15, 183, 95, 548, 3]\n",
      "[489, 488, 3, 490, 136, 75, 94, 1165]\n",
      "[95, 1166, 6, 184, 43, 59]\n",
      "[492, 491, 132, 1167, 493, 1168]\n",
      "[195, 117, 1169, 494, 3, 194, 73, 495]\n",
      "[137, 135, 15, 66, 198, 291, 40, 100]\n",
      "[496, 1170, 497, 519, 72, 112, 1171]\n",
      "[211, 101, 3, 1172, 185, 30, 498, 286]\n",
      "[195, 262, 23, 502, 268, 503]\n",
      "[1173, 504, 1174, 505, 1175, 14, 53, 506, 507]\n",
      "[508, 286, 111, 1176, 6, 1177, 193, 281]\n",
      "[12, 196, 7, 129, 26, 53, 67, 6, 101, 102, 555]\n",
      "[32, 515, 6, 516, 517, 76, 518]\n",
      "[80, 290, 6, 119, 520, 5, 1, 39]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[137, 135, 15, 66, 198, 291, 40, 100]\n",
      "[12, 196, 7, 129, 26, 53, 67, 6, 101, 102, 555]\n",
      "[521, 328, 42, 150, 6, 287, 1178, 522]\n",
      "[52, 553, 23, 1179, 10, 523, 524, 3, 138, 156, 1180]\n",
      "[40, 525, 172, 526, 72, 554, 5, 64, 1181]\n",
      "[138, 102, 1182, 1183, 7, 529, 528]\n",
      "[530, 10, 552, 1184, 531, 532]\n"
     ]
    }
   ],
   "source": [
    "article_train = build_data_int(article,word_dict)\n",
    "title_train = build_data_int(title,word_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 添加 <start> 和 <end>\n",
    "def encode(article,title):\n",
    "    article_indices = [vocab_size] + words2ids(article,word_dict) + [vocab_size + 1]\n",
    "    title_indices = [vocab_size] + words2ids(title,word_dict) + [vocab_size + 1]\n",
    "    \n",
    "    return article_indices, title_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article BOS index is :1185\n",
      "Article EOS index is :1186\n",
      "Title BOS index is :1185\n",
      "Title EOS index is :1186\n",
      "australia s current account deficit shrunk by a record . billion dollars lrb . billion us rrb in the june quarter due to soaring commodity prices , figures released monday showed . australian current account deficit narrows sharply\n",
      "\n",
      "[1185, 556, 10, 292, 293, 200, 557, 17, 4, 558, 1, 83, 39, 201, 1, 83, 12, 202, 3, 0, 559, 560, 67, 6, 561, 139, 18, 2, 562, 294, 13, 295, 1, 1186] [1185, 85, 292, 293, 200, 1083, 224, 1186]\n"
     ]
    }
   ],
   "source": [
    "article_indices, title_indices = encode(article[0],title[0])\n",
    "\n",
    "print ('Article BOS index is :{}'.format(vocab_size))\n",
    "print ('Article EOS index is :{}'.format(vocab_size + 1))\n",
    "\n",
    "print ('Title BOS index is :{}'.format(vocab_size))\n",
    "print ('Title EOS index is :{}'.format(vocab_size + 1))\n",
    "\n",
    "print ((article[0]),(title[0]))\n",
    "print ()\n",
    "print (article_indices,title_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_to_tensor(text):\n",
    "    text_tensor = []\n",
    "    for line in text:\n",
    "        line_tensor = tf.convert_to_tensor(line)\n",
    "        text_tensor.append(line_tensor)\n",
    "    return text_tensor\n",
    "\n",
    "article_tensor, title_tensor = data_to_tensor(article_train),data_to_tensor(title_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=255899, shape=(32,), dtype=int32, numpy=\n",
       "array([556,  10, 292, 293, 200, 557,  17,   4, 558,   1,  83,  39, 201,\n",
       "         1,  83,  12, 202,   3,   0, 559, 560,  67,   6, 561, 139,  18,\n",
       "         2, 562, 294,  13, 295,   1])>"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article_tensor[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=255999, shape=(6,), dtype=int32, numpy=array([  85,  292,  293,  200, 1083,  224])>"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "title_tensor[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_length = 0\n",
    "for i in range(len(article_tensor)):\n",
    "    if len(article_tensor[i]) > max_length:\n",
    "        max_length = len(article_tensor[i])\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_tensor = keras.preprocessing.sequence.pad_sequences(article_tensor,maxlen = max_length,padding = 'post')\n",
    "title_tensor = keras.preprocessing.sequence.pad_sequences(title_tensor,maxlen = max_length,padding = 'post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((10, 42), (10, 42)), types: (tf.int32, tf.int32)>"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices((article_tensor,title_tensor)).shuffle(buffer_size)\n",
    "dataset = dataset.batch(batch_size,drop_remainder = True)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[ 19 645 221  35 646 647   9 112   3 342  28 648 649 650  21   4 113 651\n",
      "  343 652   7   4 653  51   2  74   9 156 157  11  13   1   0   0   0   0\n",
      "    0   0   0   0   0   0]\n",
      " [144  31  44  24   1  20  13   7  56  51 210   9 304 305 306  27 307 308\n",
      "  108 309 310   2  45  11   1   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0]\n",
      " [ 22 203  43  84  55 103   3   4 296 297 204   7   4 563 564   3   0 565\n",
      "  566  40 205   7  13   2   0 104  11   1   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0]\n",
      " [114  69  18  44 158 159  13  15  89 222   0 344   7  34  18  27  59 160\n",
      "   15  23 345 147   0  12  41 161   2  45  11   1   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0]\n",
      " [  0 654 655 656  17  38 657 658   7 659 660   0 347 348 661   7  19  34\n",
      "  349   5  39   4 350   9   4 302 154   5   1  20   2  23  48 106 351   1\n",
      "    0   0   0   0   0   0]\n",
      " [361  69  18 227  24   1  20  13   2  15  89 680 681  37 166 120  10 682\n",
      "    7   0 362   2  45  11   1   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0]\n",
      " [556  10 292 293 200 557  17   4 558   1  83  39 201   1  83  12 202   3\n",
      "    0 559 560  67   6 561 139  18   2 562 294  13 295   1   0   0   0   0\n",
      "    0   0   0   0   0   0]\n",
      " [ 32  46   7  13 106 575 208 209   2 576 577   9 578 208 301   6 579 302\n",
      "   17 580 581 582 583   9 303 107   1   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0]\n",
      " [144  69  18  44  24   1  20  13   7  56  51 210   9 304 305 306  27 307\n",
      "  308 108 309 310   2  45  11   1   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0]\n",
      " [ 32  46  10  47 333  46 624  73 625 626  13   7   4 218 334 627  33  23\n",
      "   36 628 629   4 630   3 219  12 107  73  58 153   1   0   0   0   0   0\n",
      "    0   0   0   0   0   0]], shape=(10, 42), dtype=int32)\n",
      "\n",
      "tf.Tensor(\n",
      "[[  12  221  112   28  343 1098    3  342    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      " [ 144   31   29   24    1   20    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      " [  22  203   43  135    3   40  205 1084    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      " [ 114   31   29  158  159    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      " [1100 1101  347  348   28   34  171   22   39    4  350    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      " [ 361   31   50    3  115   60    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      " [  85  292  293  200 1083  224    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      " [ 197  535  208  301    6 1086  252    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      " [ 144   31   29   24    1   20    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      " [ 197   73 1095    6 1096  165   94   28   58    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0]], shape=(10, 42), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "article_batch, title_batch = next(iter(dataset))\n",
    "\n",
    "print (article_batch)\n",
    "print ()\n",
    "print (title_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([10, 42])"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import,print_function,division,unicode_literals\n",
    "import tensorflow as tf \n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras import layers,losses,optimizers,metrics\n",
    "import os\n",
    "\n",
    "# ----------------------------\n",
    "#  Encoder\n",
    "#-----------------------------\n",
    "\n",
    "class Encoder(keras.Model):\n",
    "\n",
    "    def __init__(self,vocab_size,embedding_dim,encoder_units,batch_size):\n",
    "        super(Encoder,self).__init__()\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        self.encoder_units = encoder_units\n",
    "        self.embedding = layers.Embedding(vocab_size,embedding_dim)\n",
    "        self.gru = layers.GRU(self.encoder_units,\n",
    "                                    return_sequences = True,\n",
    "                                    return_state = True,\n",
    "                                    recurrent_initializer = 'glorot_uniform')\n",
    "\n",
    "    def call(self,x,hidden):\n",
    "        x = self.embedding(x)\n",
    "        output,state = self.gru(x,initial_state = hidden)\n",
    "        return output,state\n",
    "    \n",
    "    def initialize_hidden_state(self):\n",
    "        return tf.zeros((self.batch_size,self.encoder_units))\n",
    "\n",
    "# ----------------------------\n",
    "#  Attention\n",
    "#-----------------------------\n",
    "class BahdanauAttention(keras.Model):\n",
    "\n",
    "    def __init__(self,units):\n",
    "        super(BahdanauAttention,self).__init__()\n",
    "\n",
    "        self.W1 = layers.Dense(units)\n",
    "        self.W2 = layers.Dense(units)\n",
    "        self.V = layers.Dense(1)\n",
    "\n",
    "    def call(self,query,values):\n",
    "        # hidden shape == (batch_size,hidden size)\n",
    "        # hidden_with_time_axis shape == (batch_size,1,hidden size)\n",
    "        hidden_with_time_axis = tf.expand_dims(query,1)\n",
    "\n",
    "        # score shape == (batch_size,max_length,1)\n",
    "        score = self.V(tf.nn.tanh(\n",
    "            self.W1(values) + self.W2(hidden_with_time_axis)\n",
    "        ))\n",
    "\n",
    "        # attention_weights shape == (batch_size,max_length,1)\n",
    "        attention_weights = tf.nn.softmax(score,axis = 1)\n",
    "\n",
    "        # context_vector shape after sum == (batch_size,hidden_size)\n",
    "        context_vector = attention_weights * values\n",
    "        context_vector = tf.reduce_sum(context_vector,axis = 1)\n",
    "\n",
    "        return context_vector,attention_weights\n",
    "\n",
    "# ----------------------------\n",
    "#  Decoder\n",
    "#-----------------------------\n",
    "class Decoder(keras.Model):\n",
    "    \n",
    "    def __init__(self,vocab_size,embedding_dim,decoder_units,batch_size):\n",
    "        super(Decoder,self).__init__()\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        self.decoder_units = decoder_units\n",
    "        self.embedding = layers.Embedding(vocab_size,embedding_dim)\n",
    "        self.gru = layers.GRU(self.decoder_units,\n",
    "                                    return_sequences = True,\n",
    "                                    return_state = True,\n",
    "                                    recurrent_initializer = 'glorot_uniform')\n",
    "        self.fc = layers.Dense(vocab_size)\n",
    "\n",
    "        self.attention = BahdanauAttention(self.decoder_units)\n",
    "\n",
    "    # encoder_output shape = (batch_size,max_length,hidden_size)\n",
    "    def call(self,x,hidden,encoder_output):\n",
    "        context_vector,attention_weights = self.attention(hidden,encoder_output)\n",
    "\n",
    "        # x shape after passing through embedding == (batch_size,1,embedding_dim)\n",
    "        x = self.embedding(x)\n",
    "\n",
    "        # x shape after concatenation == (batch_size,1,embedding_dim + hidden_size)\n",
    "        x = tf.concat([tf.expand_dims(context_vector,1),x],axis = -1)\n",
    "\n",
    "        # passing the concatenated vector to the GRU\n",
    "        output,state = self.gru(x)\n",
    "\n",
    "        # output shape == (batch_size * 1,hidden_size)\n",
    "        output = tf.reshape(output,(-1,output.shape[2]))\n",
    "\n",
    "        # output shape == (batch_size,vocab)\n",
    "        x = self.fc(output)\n",
    "\n",
    "        return x,state,attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder output shape:(batch size,sequence length units) (10, 42, 1024)\n",
      "Encoder Hidden state shape:(batch_size,units) (10, 1024)\n",
      "Attention result shape:(batch size,units) (10, 1024)\n",
      "Attention weights shape:(batch size,sequence_length,1) (10, 42, 1)\n",
      "Decoder output shape: (batch_size,vocab_size) (10, 1187)\n"
     ]
    }
   ],
   "source": [
    "vocab_size = vocab_size + 2\n",
    "# Encoder\n",
    "encoder = Encoder(vocab_size,embedding_size,lstm_size,batch_size)\n",
    "sample_hidden = encoder.initialize_hidden_state()\n",
    "sample_output,sample_hidden = encoder(article_batch,sample_hidden)\n",
    "print ('Encoder output shape:(batch size,sequence length units) {}'.format(sample_output.shape))\n",
    "print ('Encoder Hidden state shape:(batch_size,units) {}'.format(sample_hidden.shape))\n",
    "\n",
    "# Attention\n",
    "attention_layer = BahdanauAttention(10)\n",
    "attention_result,attention_weights = attention_layer(sample_hidden,sample_output)\n",
    "print (\"Attention result shape:(batch size,units) {}\".format(attention_result.shape))\n",
    "print (\"Attention weights shape:(batch size,sequence_length,1) {}\".format(attention_weights.shape))\n",
    "\n",
    "# Decoder\n",
    "decoder = Decoder(vocab_size,embedding_size,lstm_size,batch_size)\n",
    "sample_decoder_output,_,_ = decoder(tf.random.uniform((10,1)),sample_hidden,sample_output)\n",
    "print ('Decoder output shape: (batch_size,vocab_size) {}'.format(sample_decoder_output.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.Adam()\n",
    "loss_object = keras.losses.SparseCategoricalCrossentropy(from_logits=True,reduction='none')\n",
    "\n",
    "#checkpoint_path = 'training_checkpoints'\n",
    "#if not os.path.exists('training_checkpoints'):\n",
    "#    os.mkdir('training_checkpoints')\n",
    "\n",
    "#ckpt = tf.train.Checkpoint(optimizer = optimizer,\n",
    "#                           encoder = encoder,\n",
    "#                           decoder = decoder)\n",
    "#\n",
    "#ckpt_manager = tf.train.CheckpointManager(ckpt,checkpoint_path,max_to_keep=5)\n",
    "\n",
    "#if ckpt_manager.latest_checkpoint:\n",
    "#    ckpt.restore(ckpt_manager.latest_checkpoint)\n",
    "#else:\n",
    "#   print ('no saved checkpoint... ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_func(real,pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real,0))\n",
    "    loss_ = loss_object(real,pred)\n",
    "    \n",
    "    mask = tf.cast(mask,dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "    \n",
    "    return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_one_step(inp,targ,enc_hidden):\n",
    "    batch_size,targ_length = targ.shape\n",
    "    loss = 0\n",
    "    with tf.GradientTape() as tape:\n",
    "        enc_output, enc_hidden = encoder(inp,enc_hidden)\n",
    "        dec_hidden = enc_hidden\n",
    "\n",
    "        for t in range(targ_length - 1):\n",
    "            dec_input = tf.expand_dims(targ[:,t],1) # using teacher forcing\n",
    "            predictions,dec_hidden,_ = decoder(dec_input,dec_hidden,enc_output)\n",
    "            loss += loss_func(targ[:,t + 1],predictions)\n",
    "\n",
    "    batch_loss = loss / targ_length\n",
    "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "    gradients = tape.gradient(loss,variables)\n",
    "    optimizer.apply_gradients(zip(gradients,variables))\n",
    "\n",
    "    return batch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_batch_per_epoch = (len(article_tensor) - 1) // batch_size  + 1\n",
    "num_batch_per_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).save_counter\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).decoder.gru.cell.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).decoder.gru.cell.recurrent_kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).decoder.gru.cell.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).decoder.attention.W1.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).decoder.attention.W1.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).decoder.attention.W2.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).decoder.attention.W2.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).decoder.attention.V.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).decoder.attention.V.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).encoder.gru.cell.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).encoder.gru.cell.recurrent_kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).encoder.gru.cell.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).decoder.gru.cell.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).decoder.gru.cell.recurrent_kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).decoder.gru.cell.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).decoder.attention.W1.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).decoder.attention.W1.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).decoder.attention.W2.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).decoder.attention.W2.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).decoder.attention.V.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).decoder.attention.V.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).encoder.gru.cell.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).encoder.gru.cell.recurrent_kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).encoder.gru.cell.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).decoder.gru.cell.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).decoder.gru.cell.recurrent_kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).decoder.gru.cell.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).decoder.attention.W1.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).decoder.attention.W1.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).decoder.attention.W2.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).decoder.attention.W2.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).decoder.attention.V.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).decoder.attention.V.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).encoder.gru.cell.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).encoder.gru.cell.recurrent_kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).encoder.gru.cell.bias\n",
      "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/alpha/guide/checkpoints#loading_mechanics for details.\n",
      "Epoch 1 Batch 0 Loss 1.0450\n",
      "Epoch 2 Batch 0 Loss 0.9431\n",
      "Saving checkpoint for epoch 2 at training_checkpoints\\ckpt-22\n",
      "Epoch 3 Batch 0 Loss 0.8618\n",
      "Epoch 4 Batch 0 Loss 0.8344\n",
      "Saving checkpoint for epoch 4 at training_checkpoints\\ckpt-22\n",
      "Epoch 5 Batch 0 Loss 0.8075\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    start = time.time()\n",
    "    \n",
    "    enc_hidden = encoder.initialize_hidden_state()\n",
    "    \n",
    "    total_loss = 0\n",
    "    \n",
    "    for batch,(inp,targ) in enumerate(dataset.take(num_batch_per_epoch)):\n",
    "        batch_loss = train_one_step(inp,targ,enc_hidden)\n",
    "        total_loss += batch_loss\n",
    "        \n",
    "        if batch % 10 == 0:\n",
    "            print ('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,batch,batch_loss.numpy()))\n",
    "    if (epoch + 1) % 2 ==0:\n",
    "        #ckpt_save_path = ckpt_manager.save()\n",
    "        print ('Saving checkpoint for epoch {} at {}'.format(epoch + 1,ckpt_save_path))\n",
    "        #\n",
    "print ('Epoch {} loss {:.4f}'.format(epoch + 1,total_loss / num_batch_per_epoch))\n",
    "print ('Time taken for one epoch {} sec\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_sentence = 'the united nations  humanitarian chief john holmes arrived in ethiopia monday to tour regions affected by drought , which has left some eight million people in need of urgent food aid .'\n",
    "raw_data = text_sentence\n",
    "raw_data = process_text(raw_data)\n",
    "\n",
    "raw_data_sequence= words2ids(raw_data,word_dict)\n",
    "raw_data_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_token = [vocab_size]\n",
    "end_token = [vocab_size + 1]\n",
    "\n",
    "raw_data_sequence = start_token + raw_data_sequence + end_token\n",
    "raw_data_sequence\n",
    "#raw_data_sequence = tf.convert_to_tensor(raw_data_sequence)\n",
    "#raw_data_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=201072, shape=(1, 34), dtype=int32, numpy=\n",
       "array([[1185,    0,  140,  319,  601,   57,  148,  602,  320,    3,  321,\n",
       "          13,    6,  603,  322,  604,   17,  323,    2,   71,   35,  605,\n",
       "         149,  606,  607,   84,    3,  608,    5,  609,  610,  324,    1,\n",
       "        1186]])>"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_sequence = tf.expand_dims(raw_data_sequence,0)\n",
    "input_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "indices[0,0] = 1185 is not in [0, 1185) [Op:ResourceGather] name: encoder_15/embedding_18/embedding_lookup/",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-130-5599850bc055>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0men_initial_states\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mencoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minitialize_hidden_state\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0men_outputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0men_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mencoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_sequence\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0men_initial_states\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0men_outputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;31m# Attention\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3.5.20\\envs\\tensorflow2.0\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    710\u001b[0m           with base_layer_utils.autocast_context_manager(\n\u001b[0;32m    711\u001b[0m               input_list, self._mixed_precision_policy.should_cast_variables):\n\u001b[1;32m--> 712\u001b[1;33m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    713\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle_activity_regularization\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    714\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_set_mask_metadata\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-114-688c7bb5370e>\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, x, hidden)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mhidden\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m         \u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mstate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgru\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0minitial_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhidden\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3.5.20\\envs\\tensorflow2.0\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    710\u001b[0m           with base_layer_utils.autocast_context_manager(\n\u001b[0;32m    711\u001b[0m               input_list, self._mixed_precision_policy.should_cast_variables):\n\u001b[1;32m--> 712\u001b[1;33m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    713\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle_activity_regularization\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    714\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_set_mask_metadata\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3.5.20\\envs\\tensorflow2.0\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\embeddings.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m    176\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m'int32'\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mdtype\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m'int64'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    177\u001b[0m       \u001b[0minputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'int32'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 178\u001b[1;33m     \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0membedding_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membedding_lookup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membeddings\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    179\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    180\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3.5.20\\envs\\tensorflow2.0\\lib\\site-packages\\tensorflow\\python\\ops\\embedding_ops.py\u001b[0m in \u001b[0;36membedding_lookup\u001b[1;34m(params, ids, partition_strategy, name, validate_indices, max_norm)\u001b[0m\n\u001b[0;32m    313\u001b[0m       \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    314\u001b[0m       \u001b[0mmax_norm\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 315\u001b[1;33m       transform_fn=None)\n\u001b[0m\u001b[0;32m    316\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    317\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3.5.20\\envs\\tensorflow2.0\\lib\\site-packages\\tensorflow\\python\\ops\\embedding_ops.py\u001b[0m in \u001b[0;36m_embedding_lookup_and_transform\u001b[1;34m(params, ids, partition_strategy, name, max_norm, transform_fn)\u001b[0m\n\u001b[0;32m    131\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolocate_with\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    132\u001b[0m         result = _clip(\n\u001b[1;32m--> 133\u001b[1;33m             array_ops.gather(params[0], ids, name=name), ids, max_norm)\n\u001b[0m\u001b[0;32m    134\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtransform_fn\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    135\u001b[0m           \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtransform_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3.5.20\\envs\\tensorflow2.0\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    178\u001b[0m     \u001b[1;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    179\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 180\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    181\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    182\u001b[0m       \u001b[1;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3.5.20\\envs\\tensorflow2.0\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py\u001b[0m in \u001b[0;36mgather\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m   3457\u001b[0m     \u001b[1;31m# TODO(apassos) find a less bad way of detecting resource variables without\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3458\u001b[0m     \u001b[1;31m# introducing a circular dependency.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3459\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msparse_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3460\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3461\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mgen_array_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgather_v2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3.5.20\\envs\\tensorflow2.0\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py\u001b[0m in \u001b[0;36msparse_read\u001b[1;34m(self, indices, name)\u001b[0m\n\u001b[0;32m    969\u001b[0m       \u001b[0mvariable_accessed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    970\u001b[0m       value = gen_resource_variable_ops.resource_gather(\n\u001b[1;32m--> 971\u001b[1;33m           self._handle, indices, dtype=self._dtype, name=name)\n\u001b[0m\u001b[0;32m    972\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    973\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dtype\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvariant\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3.5.20\\envs\\tensorflow2.0\\lib\\site-packages\\tensorflow\\python\\ops\\gen_resource_variable_ops.py\u001b[0m in \u001b[0;36mresource_gather\u001b[1;34m(resource, indices, dtype, batch_dims, validate_indices, name)\u001b[0m\n\u001b[0;32m    735\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    736\u001b[0m         \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 737\u001b[1;33m       \u001b[0m_six\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    738\u001b[0m   \u001b[1;31m# Add nodes to the TensorFlow graph.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    739\u001b[0m   \u001b[0mdtype\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_execute\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_type\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"dtype\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3.5.20\\envs\\tensorflow2.0\\lib\\site-packages\\six.py\u001b[0m in \u001b[0;36mraise_from\u001b[1;34m(value, from_value)\u001b[0m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: indices[0,0] = 1185 is not in [0, 1185) [Op:ResourceGather] name: encoder_15/embedding_18/embedding_lookup/"
     ]
    }
   ],
   "source": [
    "# Encoder\n",
    "encoder = Encoder(vocab_size,embedding_size,lstm_size,batch_size)\n",
    "en_initial_states = encoder.initialize_hidden_state()\n",
    "\n",
    "en_outputs,en_state = encoder(input_sequence,en_initial_states)\n",
    "en_outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
